from __future__ import annotations
import re

import pytesseract
from PIL import Image
from io import BytesIO
import json

import logging
import sys
import os
from datetime import datetime, timedelta
from typing import Any
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup
from zoneinfo import ZoneInfo
import numpy as np
import cv2
from google import genai  # ì‹ í˜• ë¼ì´ë¸ŒëŸ¬ë¦¬
from dotenv import load_dotenv
RECIPIENTS_DEFAULT = [
    {"name": "ê´€ë¦¬ì", "contact": "01026570090"} 
]
BOARDS_DEFAULT = [
    {"name": "í•™ë¶€ê³µì§€", "category": "notice_under"},
    {"name": "í•™ë¶€ì¥í•™", "category": "scholarship_under"},
    {"name": "ì •ë³´ëŒ€ì†Œì‹", "category": "news"},
    {"name": "ì·¨ì—…ì •ë³´", "category": "course_job"},
    {"name": "í”„ë¡œê·¸ë¨", "category": "course_program"},
    {"name": "ì¸í„´ì‹­", "category": "course_intern"},
    {"name": "ê³µëª¨ì „", "category": "course_competition"},
]

session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
})
load_dotenv() # .env íŒŒì¼ì„ ì½ì–´ì„œ os.getenvê°€ ê°’ì„ ì°¾ì„ ìˆ˜ ìˆê²Œ í•´ì¤Œ
logger = logging.getLogger()
logger.setLevel(logging.INFO)
pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'
BASE_URL_DEFAULT = "https://info.korea.ac.kr/info/board/"
HTTP_TIMEOUT = float(os.getenv("HTTP_TIMEOUT", "15"))
SENDER_KEY = os.getenv("KAKAO_SENDER_KEY")
SECRET_KEY = os.getenv("KAKAO_SECRET_KEY")
APP_KEY = os.getenv("KAKAO_APP_KEY")
TEMPLATE_CODE = "send-article"
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)] 
)
LOG = logging.getLogger(__name__)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "70"))
TIMEZONE = ZoneInfo("Asia/Seoul")
# [ì¶”ê°€] AI ì œê³µìë¥¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„ íƒ (ê¸°ë³¸ê°’: gemini)
AI_PROVIDER = os.getenv("AI_PROVIDER", "gemini").lower() 
# OpenAI í‚¤ë„ í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ ë¶ˆëŸ¬ì˜¤ê¸° (ë‚˜ì¤‘ì„ ìœ„í•´)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = genai.Client(api_key=GEMINI_API_KEY) if GEMINI_API_KEY else None
# app/jobs/korea_university.py ë‚´ send_kakao ìˆ˜ì •

if GEMINI_API_KEY:
    client = genai.Client(api_key=GEMINI_API_KEY)
else:
    LOG.warning("GEMINI_API_KEY is missing!")
    client = None
# ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì œì–´
# app/jobs/korea_university.py
def run(event: dict[str, Any], context: Any | None = None) -> dict[str, Any]:
    """
    ìµœì¢… ì§„ì…ì : main.pyë¡œë¶€í„° JSONì„ ë°›ì•„ ì „ í”„ë¡œì„¸ìŠ¤ë¥¼ ì œì–´í•©ë‹ˆë‹¤.
    """
    LOG.info("ğŸ“¥ [ë°ì´í„° ìˆ˜ì‹ ] í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    
    # 1. ì¸í’‹ ë°ì´í„° íŒŒì‹± ë° í”„ë¡œí•„ ìƒì„±
    user_profile = event.get("userProfile", {})
    major = user_profile.get("major", "")
    interests = ", ".join(user_profile.get("interestFields", []))
    combined_profile = f"ì „ê³µ: {major}, ê´€ì‹¬ë¶„ì•¼: {interests}"
    
    # [ì—ëŸ¬ í•´ê²°] ì‚¬ìš©ìê°€ ë³´ë‚¸ intervalDaysë¥¼ ê°€ì ¸ì™€ì„œ í•˜ìœ„ í•¨ìˆ˜ì— ì „ë‹¬ ì¤€ë¹„
    interval = user_profile.get("intervalDays", 3)
    
    target_url = event.get("targetUrl") or BASE_URL_DEFAULT
    base_url = normalize_base(target_url)
    
    # ëŒ€ìƒ ê²Œì‹œíŒ ê²°ì •
    target_boards = BOARDS_DEFAULT
    for b in BOARDS_DEFAULT:
        if b['category'] in target_url:
            target_boards = [b]
            break

    total_scanned_count = 0 
    aligned_total = []

    # --- [í†µí•©] process_board í•¨ìˆ˜ ì—†ì´ ì—¬ê¸°ì„œ ì§ì ‘ ë£¨í”„ë¥¼ ë•ë‹ˆë‹¤ ---
    for board in target_boards:
        try:
            LOG.info(f"ğŸ” {board['name']} ê²Œì‹œíŒ ë¶„ì„ ì‹œì‘...")
            
            # [Step 1] ê²Œì‹œíŒ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            page_url, html = fetch_board(base_url, board)
            
            # [Step 2] 1ì°¨ í¬ë¡¤ë§: ë‚ ì§œ í•„í„°ë§ ì ìš© (ì¸ì 3ê°œ ì •ìƒ ì „ë‹¬)
            # ì´ì œ parse_posts(html, page_url, interval) í˜•íƒœë¡œ í˜¸ì¶œë©ë‹ˆë‹¤.
            posts = parse_posts(html, page_url, interval)
            total_scanned_count += len(posts)
            
            # [Step 3] AI í‰ê°€ ë° ìƒì„¸ í¬ë¡¤ë§
            aligned, _ = evaluate_posts(combined_profile, board["name"], posts)
            aligned_total.extend(aligned)
            
        except Exception as exc:
            LOG.error(f"âŒ {board['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {exc}")
            continue

    # 2. ìƒíƒœ ì„¸ë¶„í™” ë° ê²°ê³¼ ì¡°ë¦½
    if total_scanned_count == 0:
        return {
            "status": "NO_NEW_POSTS",
            "relevanceScore": 0.0,
            "data": None,
            "message": f"ìµœê·¼ {interval}ì¼ ë™ì•ˆ ìƒˆë¡œìš´ ê³µì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        }
            
    if not aligned_total:
        return {
            "status": "NO_MATCHING_POSTS",
            "relevanceScore": 0.0,
            "data": None,
            "message": "ì‹ ê·œ ê³µì§€ëŠ” ìˆìœ¼ë‚˜ ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤."
        }

    # ì„±ê³µ ì‹œ ì ìˆ˜ ìˆœ ì •ë ¬ í›„ ë°˜í™˜
    aligned_total.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    best_post = aligned_total[0]
    
    return {
        "status": "SUCCESS",
        "relevanceScore": best_post.get("relevance_score", 0.0),
        "data": {
            "category": "ê³µì§€ì‚¬í•­",
            "title": best_post["title"],
            "sourceName": "ê³ ë ¤ëŒ€í•™êµ ì •ë³´ëŒ€í•™",
            "summary": best_post.get("reason", "ë¶„ì„ ì™„ë£Œ"),
            "originalUrl": best_post["link"],
            "timestamp": datetime.now(TIMEZONE).isoformat()
        }
    }# ì…ë ¥ë°›ì€ URLì„ í¬ë¡¤ë§í•˜ê¸° ì í•©í•œ í‘œì¤€í˜•íƒœë¡œ ë³€í™˜
def normalize_base(url: str | None) -> str: 
    if not url:
        return BASE_URL_DEFAULT
    trimmed = url.strip()
    if trimmed.endswith(".do"):
        trimmed = trimmed[: trimmed.rfind("/") + 1]
    return f"{trimmed.rstrip('/')}/"

# fetch_board(base_url, board): íŠ¹ì • ê²Œì‹œíŒ ì¹´í…Œê³ ë¦¬ì˜ URLì„ ìƒì„±í•˜ê³  í•´ë‹¹ í˜ì´ì§€ì˜ HTML ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
def fetch_board(base_url: str, board: dict[str, str]) -> tuple[str, str]:
    page_url = f"{base_url}{board['category']}.do"
    resp = session.get(page_url, timeout=HTTP_TIMEOUT)
    resp.raise_for_status()
    return page_url, resp.text
# HTMLì—ì„œ ê³µì§€ì‚¬í•­ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. interval_daysë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ ë‚ ì§œì˜ ê¸€ì´ ë‚˜ì˜¤ë©´ ì¦‰ì‹œ ì¤‘ë‹¨(break)í•˜ì—¬ ë¶ˆí•„ìš”í•œ íƒìƒ‰ì„ ë°©ì§€í•©ë‹ˆë‹¤. 
def parse_posts(html: str, page_url: str, interval_days: int) -> list[dict[str, str]]:
    soup = BeautifulSoup(html, "html.parser")
    today = datetime.now(TIMEZONE).date()
    
    # LOOKBACK_DAYS ëŒ€ì‹  ë„˜ê²¨ë°›ì€ interval_days ì‚¬ìš©
    cutoff = today - timedelta(days=interval_days - 1)
    posts: list[dict[str, str]] = []
    
    for row in soup.select("tr"):
        cells = row.find_all("td")
        if not cells:
            continue
        
        # ë‚ ì§œ íŒŒì‹± (ê³ ë ¤ëŒ€ í˜•ì‹: YYYY.MM.DD)
        date_text = cells[-1].get_text(strip=True)
        try:
            row_date = datetime.strptime(date_text, "%Y.%m.%d").date()
        except ValueError:
            continue
            
        if row_date < cutoff:
            continue
            
        link_tag = row.select_one("a.article-title")
        if not link_tag:
            continue
            
        href = (link_tag.get("href") or "").replace("amp;", "")
        title = link_tag.get_text(strip=True)
        posts.append({"title": title, "link": urljoin(page_url, href)})
        
    return posts

# ìˆ˜ì§‘ëœ ëª©ë¡ì„ ìˆœíšŒí•˜ë©° AI ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ê¸°ì¤€ì¹˜(THRESHOLD) ì´ìƒì¸ ê²Œì‹œë¬¼ë§Œ ìƒì„¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
def evaluate_posts(profile_text: str, board_name: str, posts: list[dict[str, str]]) -> tuple[list[dict[str, Any]], list[dict[str, Any]]]:
    LOG.info(f"Evaluating posts for board: {board_name} with {len(posts)} posts")
    aligned: list[dict[str, Any]] = []
    evaluated: list[dict[str, Any]] = []
    THRESHOLD = 0.7
    for post in posts:
        post_copy = dict(post)
        score, rationale = score_notice(profile_text, post_copy["title"], post_copy["link"])
        post_copy["reason"] = rationale
        post_copy["relevance_score"] = score # ì‹¤ì œ ì ìˆ˜ ì €ì¥
        
        # í•„ë“œ ì´ˆê¸°í™”
        post_copy["full_content"] = ""
        post_copy["images"] = []

        if score >= THRESHOLD:
            LOG.info(f"âœ… ì í•© íŒì •({score}ì ): {post_copy['title']}")            
            full_text, img_urls = fetch_post_content(post_copy["link"])
            
            ocr_combined_text = ""
            for idx, url in enumerate(img_urls):
                # ì´ë¯¸ì§€ë³„ë¡œ ìˆœë²ˆê³¼ ë§í¬ë¥¼ ë¡œê·¸ì— ë‚¨ê¹€
                ocr_result = extract_text_from_image(url, post_copy["link"])
                if ocr_result:
                    ocr_combined_text += f"\n\n--- [ì´ë¯¸ì§€ #{idx+1} í…ìŠ¤íŠ¸ ì‹œì‘] ---\n{ocr_result}\n--- [ì´ë¯¸ì§€ #{idx+1} í…ìŠ¤íŠ¸ ë] ---\n"
            
            # ìµœì¢… ê²°í•© ë° í• ë‹¹
            post_copy["full_content"] = (full_text + ocr_combined_text).strip()
            post_copy["images"] = img_urls

            # ë¡œê·¸ë¡œ ê²°í•© ê²°ê³¼ í™•ì¸
            LOG.info(f"ğŸ“Š [ê²°í•© ì™„ë£Œ] {post_copy['title']}")
            LOG.info(f"   â”” ë³¸ë¬¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_text)}")
            LOG.info(f"   â”” ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸ ê¸¸ì´: {len(ocr_combined_text)}")
            LOG.info(f"   â”” ìµœì¢… full_content ê¸¸ì´: {len(post_copy['full_content'])}")
            aligned.append(post_copy)
            
        evaluated.append(post_copy)
        print('eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', post_copy)
    return aligned, evaluated
# ìœ ì €ì˜ ì „ê³µ(major)ê³¼ ê´€ì‹¬ ë¶„ì•¼(interestFields)ë¥¼ ë°˜ì˜í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ AIì—ê²Œ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
def score_notice(profile_text: str, title: str, link: str) -> tuple[float, str]:
    if not profile_text: return 0.0, "no-profile"
    
    # [ìˆ˜ì •] AIì—ê²Œ ì ìˆ˜(0~1)ë¥¼ ì§ì ‘ ìš”êµ¬í•˜ì—¬ relevanceScore ìƒì„±
    user_prompt = f"""
    Profile: {profile_text}
    Notice Title: {title}
    Analyze how relevant this notice is to the profile. 
    Respond with a JSON object: {{"score": float, "reason": "short explanation in Korean"}}
    The score must be between 0.0 and 1.0.
    Respond ONLY with a valid JSON object. Do not include markdown code blocks
    """
    return ask_ai(user_prompt)
    
    try:
        response_text = ask_ai(user_prompt)
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ê°€ì¥ ê°„ë‹¨í•œ ë°©ì‹)
        start = response_text.find('{')
        end = response_text.rfind('}') + 1
        res_json = json.loads(response_text[start:end])
        return float(res_json.get("score", 0.0)), res_json.get("reason", "ë¶„ì„ ì™„ë£Œ")
    except:
        return 0.0, "AI ë¶„ì„ ì‹¤íŒ¨"
# genai í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ Gemini APIë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
def ask_ai(prompt: str) -> tuple[float, str]:
    try:
        LOG.info("=== [AI CALL START] ===")
        
        # 1. í”„ë¡¬í”„íŠ¸ ìœ ë‹ˆì½”ë“œ ì•ˆì „í™” (UTF-8 ê°•ì œ)
        # ë§Œì•½ promptê°€ ìœ ë‹ˆì½”ë“œê°€ ì•„ë‹ˆë¼ë©´ ê°•ì œë¡œ utf-8ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        if isinstance(prompt, bytes):
            safe_prompt = prompt.decode('utf-8')
        else:
            safe_prompt = str(prompt)

        if not client:
            LOG.error("âŒ ì—ëŸ¬: Gemini Clientê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 0.0, "no-client"

        # 2. Gemini ëª¨ë¸ í˜¸ì¶œ (168ë¼ì¸ ë¶€ê·¼)
        LOG.info(f"ğŸ¤– Calling model: gemini-2.0-flash... (Prompt size: {len(safe_prompt)})")
        # [í•µì‹¬] ëŸ°íƒ€ì„ì—ì„œ ì¸ì½”ë”© ì—ëŸ¬ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ 
        # ì‹œìŠ¤í…œ í™˜ê²½ì´d ê¹¨ì ¸ìˆì–´ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ UTF-8ì„ ì‚¬ìš©í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=safe_prompt, 
            config={
                'tools': [],
                'automatic_function_calling': {'disable': True}
            }
        )
        print(4)
        # 3. ì‘ë‹µ ì²˜ë¦¬ ë° ë¡œê·¸ ì¶œë ¥ ì‹œ ì¸ì½”ë”© ë°©ì–´
        # response.textê°€ í•œê¸€ì¼ ë•Œ LOG.infoì—ì„œ í„°ì§€ëŠ” ê²ƒì„ repr()ë¡œ ë°©ì–´í•©ë‹ˆë‹¤.
        raw_text = response.text if response.text else ""
        LOG.info(f"ğŸ“¥ Raw Response Received: {repr(raw_text)}")

        if not raw_text.strip():
            LOG.warning("âš ï¸ AI ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return 0.0, "empty-response"

        # 4. JSON íŒŒì‹±
        LOG.info("ğŸ§© Parsing JSON from response...")
        json_match = re.search(r"\{.*\}", raw_text, re.DOTALL)
        
        if json_match:
            clean_json = json_match.group(0)
            data = json.loads(clean_json)
            score = float(data.get("score", 0.0))
            reason = data.get("reason", "ë¶„ì„ ì™„ë£Œ")
            
            # ì‚¬ìœ (reason) ì¶œë ¥ ì‹œì—ë„ repr() ì‚¬ìš©
            LOG.info(f"ğŸ¯ Analysis Result - Score: {score}, Reason: {repr(reason)}")
            LOG.info("=== [AI CALL SUCCESS] ===")
            return score, reason
        else:
            LOG.error(f"âŒ JSON íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë¬¸: {repr(raw_text)}")
            raise ValueError("JSON format not found in response")

    except Exception as e:
        # ì—ëŸ¬ ë©”ì‹œì§€ ìì²´(ì˜ˆ: 'ë³¸ì¸ì˜_í‚¤')ë¥¼ ì¶œë ¥í•˜ë‹¤ í„°ì§€ì§€ ì•Šê²Œ repr(e) ì²˜ë¦¬
        LOG.error(f"ğŸ’¥ Critical Error in ask_ai: {repr(e)}")
        import traceback
        LOG.error(traceback.format_exc())
        return 0.0, f"failure: {repr(str(e))}"
# ì ìˆ˜ê°€ ë†’ì€ ê²Œì‹œë¬¼ì˜ ìƒì„¸ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ URL ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
def fetch_post_content(link: str) -> tuple[str, list[str]]:
    print(f"Fetching post content from: {link}")
    try:
        resp = requests.session.get(link, timeout=HTTP_TIMEOUT)
        resp.encoding = 'utf-8'
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # 1. ë³¸ë¬¸ ì˜ì—­ íƒìƒ‰ (ê°€ì¥ ì •í™•í•œ ì„ íƒì ìˆœì„œ)
        # ì •ë³´ëŒ€ ê²Œì‹œë¬¼ì€ ë³´í†µ .view-con ì•ˆì— .fr-viewê°€ ë“¤ì–´ìˆëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.
        content_area = (
                soup.select_one(".view-con") or 
                soup.select_one(".fr-view") or 
                soup.select_one("#article_text") or # ì¶”ê°€
                soup.select_one(".board-view-content") # ì¶”ê°€
            )
        
        if content_area:
            text = content_area.get_text(" ", strip=True)
            
            # 2. ì´ë¯¸ì§€ ì¶”ì¶œ (ë³´ì—¬ì£¼ì‹  íƒœê·¸ êµ¬ì¡° ë°˜ì˜)
            img_tags = content_area.find_all("img")
            img_urls = []
            
            for img in img_tags:
                # srcì™€ data-pathë¥¼ ëª¨ë‘ í™•ì¸
                src = img.get("src") or img.get("data-path")
                
                if src:
                    # í•„í„°ë§: ì—ë””í„° ì•„ì´ì½˜ì´ë‚˜ ì•„ì£¼ ì‘ì€ ì´ë¯¸ì§€ëŠ” ì œì™¸ (OCR íš¨ìœ¨ì„±)
                    if any(x in src for x in ["/icon/", "base64", "emoji"]):
                        continue
                    
                    # ìƒëŒ€ ê²½ë¡œ(/_res/...)ë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ê²°í•©
                    # urljoinì€ linkê°€ https://info.korea.ac.kr/... ì´ë¯€ë¡œ ì•Œì•„ì„œ í•©ì³ì¤ë‹ˆë‹¤.
                    full_url = urljoin(link, src)
                    img_urls.append(full_url)
            
            LOG.info(f"âœ… ì´ë¯¸ì§€ ê°ì§€ ì„±ê³µ: {len(img_urls)}ê°œ ë°œê²¬ (URL: {link})")
            return text, img_urls
            
        LOG.warning(f"âš ï¸ ë³¸ë¬¸ ì˜ì—­ íƒìƒ‰ ì‹¤íŒ¨: {link}")
        return "ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
        
    except Exception as e:
        LOG.error(f"âŒ 2ì°¨ í¬ë¡¤ë§ ì—ëŸ¬: {e}")
        return f"ì—ëŸ¬ ë°œìƒ: {e}", []
    
def extract_text_from_image(img_url: str, parent_link: str) -> str:
    try:
        resp = session.get(img_url, timeout=HTTP_TIMEOUT)
        # ë¡œê·¸ì— ì›ë³¸ ê²Œì‹œê¸€ ë§í¬(parent_link)ë¥¼ í¬í•¨í•˜ì—¬ ì¶œë ¥
        LOG.info(f"ğŸ“¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„: {img_url} (ì¶œì²˜: {parent_link})")
        LOG.info(f"   â”” ì‘ë‹µ: {resp.status_code}, íƒ€ì…: {resp.headers.get('Content-Type')}")

        if "image" not in resp.headers.get("Content-Type", "").lower():
            LOG.error(f"   â”” ì‹¤íŒ¨: ì´ë¯¸ì§€ê°€ ì•„ë‹˜ ({img_url})")
            return ""

        img = Image.open(BytesIO(resp.content))
        processed = preprocess_for_ocr(img)

        text = pytesseract.image_to_string(
            processed,
            lang="kor+eng",
            config="--oem 3 --psm 6"
        )
        LOG.info(f"   â”” OCR ì²˜ë¦¬ ì™„ë£Œ (ê¸€ì ìˆ˜: {len(text.strip())})")
        return text.strip()
    except Exception as e:
        LOG.error(f"   â”” OCR ì‹¤íŒ¨ ({img_url}): {e}")
        return ""  
def preprocess_for_ocr(pil_img: Image.Image) -> Image.Image:
    img = np.array(pil_img)
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    _, thresh = cv2.threshold(gray, 180, 255, cv2.THRESH_BINARY)
    return Image.fromarray(thresh)
def send_kakao(contact: str, template_code: str, template_param: dict[str, str]) -> dict[str, Any]:
    payload = {
        "senderKey": SENDER_KEY,
        "templateCode": template_code,
        "recipientList": [{"recipientNo": contact, "templateParameter": template_param}],
    }
    headers = {"X-Secret-Key": SECRET_KEY, "Content-Type": "application/json;charset=UTF-8"}
    url = f"https://api-alimtalk.cloud.toast.com/alimtalk/v2.2/appkeys/{APP_KEY}/messages"
    
    try:
        # [ìˆ˜ì •] POST ìš”ì²­ì´ ë¨¼ì € ì™€ì•¼ í•©ë‹ˆë‹¤.
        resp = session.post(url, json=payload, headers=headers, timeout=HTTP_TIMEOUT)
        # [ìˆ˜ì •] ê·¸ í›„ì— ë¡œê·¸ë¥¼ ì°ì–´ì•¼ NameErrorê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        LOG.info(f"Kakao API ì‘ë‹µ ìƒíƒœ: {resp.status_code}")
        LOG.info(f"Kakao API ì‘ë‹µ ë³¸ë¬¸: {resp.text}")
        if resp.status_code != 200:
            LOG.error("Kakao send failed (%s) %s", resp.status_code, resp.text)
            return {"error": "API_STATUS_ERROR", "status": resp.status_code}
            
        return resp.json() if "application/json" in resp.headers.get("Content-Type", "") else {"status": resp.status_code}
    except Exception as e:
        LOG.error("Kakao connection error: %s", e)
        return {"error": str(e)}










    results: list[dict[str, Any]] = []
    for post in posts:
        title_prefix = "[ì í•©]" if post.get("aligned") else ""
        title = f"{title_prefix} ê³ ë ¤ëŒ€ ì •ë³´ëŒ€ ê³µì§€ ({board['name']})\n\n{post['title']}"
        
        for target in recipients:
            params = {
                "korean-title": title,
                "customer-name": target["name"],
                "article-link": post["link"],
            }
            try:
                data = send_kakao(target["contact"], TEMPLATE_CODE, params)
                results.append({
                    "board": board["name"],
                    "title": post["title"],
                    "recipient": target["contact"],
                    "status": data,
                })
            except Exception as exc:
                LOG.exception("Kakao send error: %s", exc)
                results.append({
                    "board": board["name"],
                    "title": post["title"],
                    "recipient": target["contact"],
                    "error": str(exc),
                })
    return results


    try:
        page_url, html = fetch_board(base_url, board)
        posts = parse_posts(html, page_url)
        aligned, evaluated = evaluate_posts(profile_text, board["name"], posts)

        print(profile_text,"ddddddddddddddddddddddddddddddddd", board["name"], posts)
        LOG.info(f"ğŸ“ {board['name']} í‰ê°€ ì™„ë£Œ: ì´ {len(posts)}ê±´ ì¤‘ {len(aligned)}ê±´ ì í•©")
    except Exception as exc:
        LOG.info("Board fetch error for %s: %s", board["name"], exc)

        return {"board": board["name"], "error": str(exc), "posts": [], "sent": [], "evaluated": []}
    
    # [ì„¤ì •] ì¹´ì¹´ì˜¤ ì „ì†¡ì„ ì ì‹œ ë§‰ê³  ì‹¶ì„ ë•Œ ì•„ë˜ë¥¼ ì£¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    # sent = notify(board, aligned, recipients) TODO 
    sent = [] 
    LOG.info(f"ğŸ“¢ [ì „ì†¡ ìŠ¤í‚µ] {board['name']} ì í•© ê³µì§€ {len(aligned)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    
    return {"board": board["name"], "posts": aligned, "sent": sent, "evaluated": evaluated}

# í¬ë¡¤ë§ ëŒ€ìƒ ê²Œì‹œíŒ ì •ì˜ (ì½”ë“œ ìƒë‹¨ì— ì—†ë‹¤ë©´ ì¶”ê°€í•˜ì„¸ìš”)

if __name__ == "__main__":
    # 1. ë¡œê·¸ ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    LOG.info("ğŸš€ event.jsonì„ ì´ìš©í•œ ë¡œì»¬ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 2. event.json íŒŒì¼ ì½ê¸°
    event_path = "event.json"
    if os.path.exists(event_path):
        with open(event_path, "r", encoding="utf-8") as f:
            try:
                event_data = json.load(f)
                LOG.info("âœ… event.json íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except json.JSONDecodeError:
                LOG.error("âŒ event.json íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                sys.exit(1)
    else:
        # íŒŒì¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ìµœì†Œí•œì˜ ë”ë¯¸ ë°ì´í„°
        LOG.warning("âš ï¸ event.jsonì´ ì—†ì–´ ê¸°ë³¸ ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        event_data = {
            "userId": "test_user",
            "targetUrl": "https://info.korea.ac.kr/info/board/notice_under.do",
            "userProfile": {
                "summary": "ê³ ë ¤ëŒ€í•™êµ ì»´í“¨í„°í•™ê³¼ í•™ìƒ, AI í•´ì»¤í†¤ ë° ì¥í•™ê¸ˆì— ê´€ì‹¬ ìˆìŒ"
            },
            "config": {"language": "Korean"}
        }

    # 3. ì‹¤ì œ run í•¨ìˆ˜ ì‹¤í–‰
    try:
        # ìš°ë¦¬ê°€ ì •ì˜í•œ ì¸í’‹/ì•„ì›ƒí’‹ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” run í•¨ìˆ˜ í˜¸ì¶œ
        final_output = run(event_data)
        
        # 4. ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ìµœì¢… API ì‘ë‹µ ê²°ê³¼ (Output):")
        print(json.dumps(final_output, ensure_ascii=False, indent=2))
        print("="*50)
        
    except Exception as e:
        LOG.error(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")