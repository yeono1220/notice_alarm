from __future__ import annotations
import re

import pytesseract
from PIL import Image
from io import BytesIO
import json
import logging
import sys
import os
from datetime import datetime, timedelta
from typing import Any
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup
from zoneinfo import ZoneInfo
import numpy as np
import cv2
from google import genai  # ì‹ í˜• ë¼ì´ë¸ŒëŸ¬ë¦¬
from dotenv import load_dotenv
RECIPIENTS_DEFAULT = [
    {"name": "ê´€ë¦¬ì", "contact": "01026570090"} 
]
BOARDS_DEFAULT = [
    {"name": "í•™ë¶€ê³µì§€", "category": "notice_under"},
    {"name": "í•™ë¶€ì¥í•™", "category": "scholarship_under"},
    {"name": "ì •ë³´ëŒ€ì†Œì‹", "category": "news"},
    {"name": "ì·¨ì—…ì •ë³´", "category": "course_job"},
    {"name": "í”„ë¡œê·¸ë¨", "category": "course_program"},
    {"name": "ì¸í„´ì‹­", "category": "course_intern"},
    {"name": "ê³µëª¨ì „", "category": "course_competition"},
]

session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
})
load_dotenv() # .env íŒŒì¼ì„ ì½ì–´ì„œ os.getenvê°€ ê°’ì„ ì°¾ì„ ìˆ˜ ìˆê²Œ í•´ì¤Œ
logger = logging.getLogger()
logger.setLevel(logging.INFO)
pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'
BASE_URL_DEFAULT = "https://info.korea.ac.kr/info/board/"
HTTP_TIMEOUT = float(os.getenv("HTTP_TIMEOUT", "15"))
SENDER_KEY = os.getenv("KAKAO_SENDER_KEY")
SECRET_KEY = os.getenv("KAKAO_SECRET_KEY")
APP_KEY = os.getenv("KAKAO_APP_KEY")
TEMPLATE_CODE = "send-article"
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)] 
)
LOG = logging.getLogger(__name__)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "70"))
TIMEZONE = ZoneInfo("Asia/Seoul")
# [ì¶”ê°€] AI ì œê³µìë¥¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„ íƒ (ê¸°ë³¸ê°’: gemini)
AI_PROVIDER = os.getenv("AI_PROVIDER", "gemini").lower() 
# OpenAI í‚¤ë„ í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ ë¶ˆëŸ¬ì˜¤ê¸° (ë‚˜ì¤‘ì„ ìœ„í•´)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = genai.Client(api_key=GEMINI_API_KEY) if GEMINI_API_KEY else None
# app/jobs/korea_university.py ë‚´ send_kakao ìˆ˜ì •
if GEMINI_API_KEY:
    client = genai.Client(api_key=GEMINI_API_KEY)
else:
    LOG.warning("GEMINI_API_KEY is missing!")
    client = None
# ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì œì–´
# app/jobs/korea_university.py
def run(event: dict[str, Any], context: Any | None = None) -> dict[str, Any]:
    LOG.info("ğŸ“¥ [ë°ì´í„° ìˆ˜ì‹ ] í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    
    # 1. ì¸í’‹ ë°ì´í„° íŒŒì‹±
    user_profile = event.get("userProfile", {})
    major = user_profile.get("major", "ì»´í“¨í„°í•™ê³¼")  # ê¸°ë³¸ê°’ ì„¤ì •
    interest_list = user_profile.get("interestFields", [])

    if not interest_list:
        interest_list = ["AI", "ì±„ìš©", "ì¥í•™ê¸ˆ", "ì¸í„´ì‹­"] # ê¸°ë³¸ ê´€ì‹¬ì‚¬ ì„¤ì •

    interests = ", ".join(interest_list)
    combined_profile = f"ì „ê³µ: {major}, ê´€ì‹¬ë¶„ì•¼: {interests}"

    LOG.info(f"ğŸ‘¤ ë¶„ì„ìš© í”„ë¡œí•„ ìƒì„± ì™„ë£Œ: {combined_profile}") # ë¡œê·¸ë¡œ í™•ì¸ í•„ìˆ˜!    
    # ì„¤ì •ê°’ ë¡œë“œ
    interval = user_profile.get("intervalDays", 30)
    raw_url = event.get("targetUrl") or BASE_URL_DEFAULT
    # ì£¼ì†Œë¥¼ ë¬´ì¡°ê±´ '.../board/' í˜•íƒœë¡œ ì •ê·œí™”
    base_url = normalize_base(raw_url)
    
    all_board_results = []
    all_final_data = [] # ëª¨ë“  ê²Œì‹œíŒì˜ ì¶”ì²œ ê³µì§€ë¥¼ ëª¨ì„ ë¦¬ìŠ¤íŠ¸
    total_found_posts = 0
    total_scanned = 0
    # 2. ê° ê²Œì‹œíŒì„ 'ë°°ì¹˜ ë°©ì‹'ìœ¼ë¡œ í•œ ë²ˆë§Œ ìˆœíšŒ
    for board in BOARDS_DEFAULT:
        try:
            LOG.info(f"ğŸš€ {board['name']} ê²Œì‹œíŒ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘ (ê¸°ê°„: {interval}ì¼)")
            
            # [ìˆ˜ì •] ì‹ ê·œ ë°°ì¹˜ í•¨ìˆ˜ë§Œ í˜¸ì¶œí•©ë‹ˆë‹¤. 
            # (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ fetch_board, parse_posts, ë°°ì¹˜ AI ë¶„ì„, ì•Œë¦¼ê¹Œì§€ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„)
            result = process_board_batch(board, base_url, combined_profile, RECIPIENTS_DEFAULT, interval)
            
            all_board_results.append(result)
            if result.get("status") == "SUCCESS":
                all_final_data.extend(result.get("data", []))
                total_scanned += 1            # ê²€ìƒ‰ëœ í¬ìŠ¤íŠ¸ ìˆ˜ í•©ì‚° (ê²°ê³¼ ë©”ì‹œì§€ìš©)
                
        except Exception as exc:
            LOG.error(f"âŒ {board['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {exc}")
            continue

    # 3. ìµœì¢… ìƒíƒœ ë°˜í™˜
    if not all_final_data:
        return {
            "status": "NO_MATCHING_POSTS",
            "message": f"ìµœê·¼ {interval}ì¼ ë™ì•ˆ ë¶„ì„ì„ ì™„ë£Œí–ˆìœ¼ë‚˜, ì¶”ì²œí• ë§Œí•œ ìƒˆ ê³µì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    return {
        "status": "SUCCESS",
        "total_boards": total_scanned,
        "recommend_count": len(all_final_data),
        "data": all_final_data # ì—¬ê¸°ì— AIê°€ ìš”ì•½í•œ ì§„ì§œ ë°ì´í„°ê°€ ë‹´ê¹ë‹ˆë‹¤!
    }
def normalize_base(url: str | None) -> str: 
    if not url:
        return BASE_URL_DEFAULT
    trimmed = url.strip()
    if trimmed.endswith(".do"):
        trimmed = trimmed[: trimmed.rfind("/") + 1]
    return f"{trimmed.rstrip('/')}/"

# fetch_board(base_url, board): íŠ¹ì • ê²Œì‹œíŒ ì¹´í…Œê³ ë¦¬ì˜ URLì„ ìƒì„±í•˜ê³  í•´ë‹¹ í˜ì´ì§€ì˜ HTML ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
def fetch_board(base_url: str, board: dict[str, str]) -> tuple[str, str]:
    page_url = f"{base_url}{board['category']}.do"
    resp = session.get(page_url, timeout=HTTP_TIMEOUT)
    resp.raise_for_status()
    return page_url, resp.text

def extract_text_from_image(img_url: str) -> str:
    """ì´ë¯¸ì§€ URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¸ì 1ê°œë¡œ í†µì¼)"""
    try:
        resp = session.get(img_url, timeout=10)
        img = Image.open(BytesIO(resp.content))
        return pytesseract.image_to_string(img, lang="kor+eng").strip()
    except Exception:
        return ""

def fetch_post_content(link: str) -> str:
    """ë³¸ë¬¸ê³¼ OCR í…ìŠ¤íŠ¸ë¥¼ í•©ì³ì„œ ë°˜í™˜"""
    try:
        resp = session.get(link, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, "html.parser")
        content_area = soup.select_one(".view-con") or soup.select_one(".fr-view")
        
        if not content_area: return ""
        
        basic_text = content_area.get_text(" ", strip=True)
        img_tags = content_area.find_all("img")
        ocr_text = ""
        for img in img_tags:
            src = img.get("src")
            if src:
                ocr_text += "\n" + extract_text_from_image(urljoin(link, src))
        
        return (basic_text + ocr_text).strip()
    except Exception as e:
        LOG.error(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

# --- í•µì‹¬: íŒ€ì¥ë‹˜ ìŠ¤íƒ€ì¼ì˜ ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ ---
def process_board_batch(board, base_url, profile_text, recipients, interval):
    try:
        # 1. 1ì°¨ í¬ë¡¤ë§ (ëª©ë¡ ìˆ˜ì§‘)
        page_url, html = fetch_board(base_url, board)
        posts = parse_posts(html, page_url, interval) 
        
        if not posts: 
            return {"board": board['name'], "status": "NO_POSTS", "posts_count": 0}

        # 2. [ë°°ì¹˜ í˜¸ì¶œ 1] ì œëª© ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
        titles_block = "\n".join([f"{i}. {p['title']}" for i, p in enumerate(posts)])
        filter_prompt = f"""
        [ê²½ê³ : ë°˜ë“œì‹œ ì¤€ìˆ˜] 
        1. ëŒ€í™” ê¸ˆì§€, ì„¤ëª… ê¸ˆì§€. 
        2. ì˜¤ì§ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹([ë²ˆí˜¸, ë²ˆí˜¸])ë§Œ ì¶œë ¥í•´.
        3. ì˜ˆ: [1, 3]

        ì‚¬ìš©ì í”„ë¡œí•„: {profile_text}
        ëª©ë¡:
        {titles_block}

        ë²ˆí˜¸: """

        filter_res_raw = ask_ai(filter_prompt)
        
        # ë°ì´í„° íƒ€ì… ë°©ì–´ (ë¦¬ìŠ¤íŠ¸/íŠœí”Œ/ë¬¸ìì—´ ì²˜ë¦¬)
        if isinstance(filter_res_raw, list):
            selected_indices = filter_res_raw
        else:
            filter_res_str = str(filter_res_raw[0] if isinstance(filter_res_raw, tuple) else filter_res_raw)
            selected_indices = [int(i) for i in re.findall(r'\d+', filter_res_str)]

        if not selected_indices:
            return {"board": board['name'], "status": "NO_MATCH", "posts_count": len(posts)}

        # 3. ì„ íƒëœ ê³µì§€ë§Œ 2ì°¨ í¬ë¡¤ë§ (ë³¸ë¬¸/OCR ìˆ˜ì§‘)
        targeted_data = []
        for idx in selected_indices:
            if idx < len(posts):
                # fetch_post_contentê°€ 2ê°œì˜ ê°’ì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì • (ë‚´ìš©, ì´ë¯¸ì§€ëª©ë¡)
                # ë§Œì•½ ì—ëŸ¬ê°€ ë‚œë‹¤ë©´ content = fetch_post_content(...)ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.
                content_res = fetch_post_content(posts[idx]['link'])
                content = content_res[0] if isinstance(content_res, tuple) else content_res
                
                targeted_data.append({
                    "title": posts[idx]['title'], 
                    "link": posts[idx]['link'], 
                    "content": content
                })

        # 4. [ë°°ì¹˜ í˜¸ì¶œ 2] í†µí•© ìš”ì•½
        summary_input = ""
        for i, d in enumerate(targeted_data):
            summary_input += f"\n[ID:{i}]\nì œëª©: {d['title']}\në³¸ë¬¸: {d['content']}\n"

        summary_prompt = f"""
        ì‚¬ìš©ì í”„ë¡œí•„({profile_text})ì— ë§ì¶° ë‹¤ìŒ ê³µì§€ë“¤ì„ ê°ê° ìš”ì•½í•´ì¤˜. 
        ë°˜ë“œì‹œ ì•„ë˜ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´.
        [
          {{"id": ë²ˆí˜¸, "summary": "ìš”ì•½ë‚´ìš©", "title": "ì›ë³¸ì œëª©"}}
        ]
        ë‚´ìš©:
        {summary_input}
        """
        
        summaries = ask_ai(summary_prompt)

        # ë¬¸ìì—´ë¡œ ì™”ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ íŒŒì‹± ë°©ì–´
        if isinstance(summaries, str):
            try:
                match = re.search(r'(\[.*\]|\{.*\})', summaries, re.DOTALL)
                summaries = json.loads(match.group(1)) if match else []
            except:
                summaries = []

        # 5. ê°œë³„ ì•Œë¦¼ ë°œì†¡
        sent_count = 0
        if isinstance(summaries, list):
            for s in summaries:
                # ì œëª© ë§¤ì¹­ìœ¼ë¡œ ì›ë³¸ ë§í¬ ì°¾ê¸°
                target_title = s.get('title', '')
                original_post = next((p for p in targeted_data if target_title in p['title']), None)
                article_link = original_post['link'] if original_post else ""
                
                for target in recipients:
                    params = {
                        "korean-title": f"[{board['name']}] {target_title}",
                        "customer-name": target["name"],
                        "article-link": article_link,
                        "summary": s.get('summary', 'ë‚´ìš© ìš”ì•½ ì‹¤íŒ¨')
                    }
                    send_kakao(target["contact"], TEMPLATE_CODE, params)
                    sent_count += 1

        return {
            "board": board['name'], 
            "status": "SUCCESS", 
            "posts_count": len(posts), 
            "matched_count": len(summaries) if isinstance(summaries, list) else 0,
            "sent_count": sent_count
        }

    except Exception as e:
        LOG.exception(f"âŒ {board['name']} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        return {"board": board['name'], "status": "ERROR", "error": str(e), "posts_count": 0}
    # HTMLì—ì„œ ê³µì§€ì‚¬í•­ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. interval_daysë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ ë‚ ì§œì˜ ê¸€ì´ ë‚˜ì˜¤ë©´ ì¦‰ì‹œ ì¤‘ë‹¨(break)í•˜ì—¬ ë¶ˆí•„ìš”í•œ íƒìƒ‰ì„ ë°©ì§€í•©ë‹ˆë‹¤. 
def parse_posts(html: str, page_url: str, interval_days: int) -> list[dict[str, str]]:
    soup = BeautifulSoup(html, "html.parser")
    today = datetime.now(TIMEZONE).date()
    
    # LOOKBACK_DAYS ëŒ€ì‹  ë„˜ê²¨ë°›ì€ interval_days ì‚¬ìš©
    cutoff = today - timedelta(days=interval_days - 1)
    posts: list[dict[str, str]] = []
    
    for row in soup.select("tr"):
        cells = row.find_all("td")
        if not cells:
            continue
        
        # ë‚ ì§œ íŒŒì‹± (ê³ ë ¤ëŒ€ í˜•ì‹: YYYY.MM.DD)
        date_text = cells[-1].get_text(strip=True)
        try:
            row_date = datetime.strptime(date_text, "%Y.%m.%d").date()
        except ValueError:
            continue
            
        if row_date < cutoff:
            continue
            
        link_tag = row.select_one("a.article-title")
        if not link_tag:
            continue
            
        href = (link_tag.get("href") or "").replace("amp;", "")
        title = link_tag.get_text(strip=True)
        posts.append({"title": title, "link": urljoin(page_url, href)})
        
    return posts

# ìˆ˜ì§‘ëœ ëª©ë¡ì„ ìˆœíšŒí•˜ë©° AI ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ê¸°ì¤€ì¹˜(THRESHOLD) ì´ìƒì¸ ê²Œì‹œë¬¼ë§Œ ìƒì„¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
def evaluate_posts(profile_text: str, board_name: str, posts: list[dict[str, str]]) -> tuple[list[dict[str, Any]], list[dict[str, Any]]]:
    LOG.info(f"Evaluating posts for board: {board_name} with {len(posts)} posts")
    aligned: list[dict[str, Any]] = []
    evaluated: list[dict[str, Any]] = []
    THRESHOLD = 0.7
    for post in posts:
        post_copy = dict(post)
        score, rationale = score_notice(profile_text, post_copy["title"], post_copy["link"])
        post_copy["reason"] = rationale
        post_copy["relevance_score"] = score # ì‹¤ì œ ì ìˆ˜ ì €ì¥
        
        # í•„ë“œ ì´ˆê¸°í™”
        post_copy["full_content"] = ""
        post_copy["images"] = []

        if score >= THRESHOLD:
            LOG.info(f"âœ… ì í•© íŒì •({score}ì ): {post_copy['title']}")            
            full_text, img_urls = fetch_post_content(post_copy["link"])
            
            ocr_combined_text = ""
            for idx, url in enumerate(img_urls):
                # ì´ë¯¸ì§€ë³„ë¡œ ìˆœë²ˆê³¼ ë§í¬ë¥¼ ë¡œê·¸ì— ë‚¨ê¹€
                ocr_result = extract_text_from_image(url)
                if ocr_result:
                    ocr_combined_text += f"\n\n--- [ì´ë¯¸ì§€ #{idx+1} í…ìŠ¤íŠ¸ ì‹œì‘] ---\n{ocr_result}\n--- [ì´ë¯¸ì§€ #{idx+1} í…ìŠ¤íŠ¸ ë] ---\n"
            
            # ìµœì¢… ê²°í•© ë° í• ë‹¹
            post_copy["full_content"] = (full_text + ocr_combined_text).strip()
            post_copy["images"] = img_urls

            # ë¡œê·¸ë¡œ ê²°í•© ê²°ê³¼ í™•ì¸
            LOG.info(f"ğŸ“Š [ê²°í•© ì™„ë£Œ] {post_copy['title']}")
            LOG.info(f"   â”” ë³¸ë¬¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_text)}")
            LOG.info(f"   â”” ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸ ê¸¸ì´: {len(ocr_combined_text)}")
            LOG.info(f"   â”” ìµœì¢… full_content ê¸¸ì´: {len(post_copy['full_content'])}")
            aligned.append(post_copy)
            
        evaluated.append(post_copy)
        print('eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', post_copy)
    return aligned, evaluated
# ìœ ì €ì˜ ì „ê³µ(major)ê³¼ ê´€ì‹¬ ë¶„ì•¼(interestFields)ë¥¼ ë°˜ì˜í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ AIì—ê²Œ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
def score_notice(profile_text: str, title: str, link: str) -> tuple[float, str]:

    if not profile_text: return 0.0, "no-profile"
    
    # [ìˆ˜ì •] AIì—ê²Œ ì ìˆ˜(0~1)ë¥¼ ì§ì ‘ ìš”êµ¬í•˜ì—¬ relevanceScore ìƒì„±
    user_prompt = f"""
    Profile: {profile_text}
    Notice Title: {title}
    Analyze how relevant this notice is to the profile. 
    Respond with a JSON object: {{"score": float, "reason": "short explanation in Korean"}}
    The score must be between 0.0 and 1.0.
    Respond ONLY with a valid JSON object. Do not include markdown code blocks
    """
    return ask_ai(user_prompt)
    
    try:
        response_text = ask_ai(user_prompt)
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ê°€ì¥ ê°„ë‹¨í•œ ë°©ì‹)
        start = response_text.find('{')
        end = response_text.rfind('}') + 1
        res_json = json.loads(response_text[start:end])
        return float(res_json.get("score", 0.0)), res_json.get("reason", "ë¶„ì„ ì™„ë£Œ")
    except:
        return 0.0, "AI ë¶„ì„ ì‹¤íŒ¨"
def summarize_content(user_profile: dict, title: str, full_content: str) -> str:
    """
    [2ì°¨ ë¶„ì„] ìˆ˜ì§‘ëœ ë³¸ë¬¸ ì „ì²´ì™€ OCR í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ë§ì¶¤ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not full_content or len(full_content) < 20:
        return "ìƒì„¸ ë³¸ë¬¸ ë‚´ìš©ì´ ë¶€ì¡±í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    interests = ", ".join(user_profile.get("interestFields", []))
    
    summary_prompt = f"""
    ë‹¹ì‹ ì€ ê³µì§€ì‚¬í•­ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ê³µì§€ì‚¬í•­ ë³¸ë¬¸ì„ ì½ê³ , 
    ì‚¬ìš©ìì˜ ê´€ì‹¬ ë¶„ì•¼({interests})ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ 3ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•˜ì„¸ìš”.
    
    ê³µì§€ ì œëª©: {title}
    ê³µì§€ ë³¸ë¬¸: {full_content}
    
    ì‘ë‹µì€ ìš”ì•½ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    """
    # ask_ai í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë˜, ìš”ì•½ë¬¸ë§Œ ë°›ë„ë¡ ê°„ë‹¨íˆ ì²˜ë¦¬ (ë˜ëŠ” ì „ìš© í˜¸ì¶œ ë¡œì§ ì‘ì„±)
    # ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ ask_aiê°€ JSONì„ ê¸°ëŒ€í•˜ë¯€ë¡œ ìš”ì•½ìš©ì€ ë³„ë„ response.text ì¶”ì¶œì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=summary_prompt
    )
    return response.text.strip()


# genai í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ Gemini APIë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
def ask_ai(prompt: str) -> list | dict | str:
    """
    Google GenAI SDK ì „ìš© ask_ai (íŠœí”Œ ë° JSON ë§ˆí¬ë‹¤ìš´ ì™„ë²½ ë°©ì–´)
    """
    try:
        # 1. ëª¨ë¸ í˜¸ì¶œ (ë³¸ì¸ì˜ ëª¨ë¸ëª…ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”. ì˜ˆ: 'gemini-2.0-flash')
        response = client.models.generate_content(
            model='gemini-2.0-flash',
            contents=prompt
        )

        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ (responseê°€ íŠœí”Œë¡œ ì˜¬ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë°©ì–´ ë¡œì§)
        if isinstance(response, tuple):
            full_text = str(response[0].text)
        else:
            full_text = str(response.text)
        
        full_text = full_text.strip()

        # 3. JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ```json ... ``` ì œê±°)
        if '[' in full_text or '{' in full_text:
            # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
            match = re.search(r'(\[.*\]|\{.*\})', full_text, re.DOTALL)
            if match:
                clean_json = match.group(1)
                try:
                    return json.loads(clean_json)
                except json.JSONDecodeError:
                    LOG.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨ (ì›ë¬¸): {full_text[:100]}")
                    return full_text # ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¼ë„ ë°˜í™˜
            
        return full_text

    except Exception as e:
        LOG.error(f"ğŸ’¥ ask_ai í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        # ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•œ í”„ë¡¬í”„íŠ¸ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸, ì•„ë‹ˆë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        return [] if "ë¦¬ìŠ¤íŠ¸" in prompt or "[" in prompt else ""# ì ìˆ˜ê°€ ë†’ì€ ê²Œì‹œë¬¼ì˜ ìƒì„¸ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ URL ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
# [korea_university.py] í•¨ìˆ˜ì˜ ì²« ì¤„
def process_board_batch(board, base_url, profile_text, recipients, interval):
    try:
        # 1. 1ì°¨ í¬ë¡¤ë§ (ì „ë‹¬ë°›ì€ interval ì‚¬ìš©)
        page_url, html = fetch_board(base_url, board)
        posts = parse_posts(html, page_url, interval) 
        
        # ... (ë‚˜ë¨¸ì§€ ë¡œì§ ë™ì¼)        posts = parse_posts(html, page_url, interval) 
        
        if not posts: 
            return {"board": board['name'], "status": "NO_POSTS", "data": []}

        # 2. [ë°°ì¹˜ í˜¸ì¶œ 1] ì œëª© ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
        titles_block = "\n".join([f"{i}. {p['title']}" for i, p in enumerate(posts)])
        filter_prompt = f"í”„ë¡œí•„: {profile_text}\nëª©ë¡:\n{titles_block}\nê´€ì‹¬ ë²ˆí˜¸ë§Œ JSON ë¦¬ìŠ¤íŠ¸ë¡œ ì‘ë‹µ."

        filter_res = ask_ai(filter_prompt)
        
        # íƒ€ì… ë°©ì–´
        if isinstance(filter_res, list):
            selected_indices = filter_res
        else:
            filter_res_str = str(filter_res[0] if isinstance(filter_res, tuple) else filter_res)
            selected_indices = [int(i) for i in re.findall(r'\d+', filter_res_str)]

        if not selected_indices:
            return {"board": board['name'], "status": "NO_MATCH", "data": []}

        # 3. ì„ íƒëœ ê³µì§€ë§Œ 2ì°¨ í¬ë¡¤ë§ (ë³¸ë¬¸/OCR ìˆ˜ì§‘)
        targeted_data = []
        for idx in selected_indices:
            if idx < len(posts):
                content_res = fetch_post_content(posts[idx]['link'])
                content = content_res[0] if isinstance(content_res, tuple) else content_res
                targeted_data.append({
                    "title": posts[idx]['title'], 
                    "link": posts[idx]['link'], 
                    "content": content
                })

        # 4. [ë°°ì¹˜ í˜¸ì¶œ 2] í†µí•© ìš”ì•½
        summary_input = "".join([f"\n[ID:{i}] ì œëª©:{d['title']}\në³¸ë¬¸:{d['content']}\n" for i, d in enumerate(targeted_data)])
        summary_prompt = f"í”„ë¡œí•„({profile_text})ì— ë§ì¶° ê° ê³µì§€ë¥¼ ìš”ì•½í•´. JSON ë¦¬ìŠ¤íŠ¸ [{{'title':'', 'summary':''}}] í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ.\në‚´ìš©:\n{summary_input}"
        
        summaries = ask_ai(summary_prompt)

        # ë¬¸ìì—´ë¡œ ì™”ì„ ê²½ìš° íŒŒì‹± ì‹œë„
        if isinstance(summaries, str):
            try:
                match = re.search(r'(\[.*\]|\{.*\})', summaries, re.DOTALL)
                summaries = json.loads(match.group(1)) if match else []
            except:
                summaries = []

        # 5. ê²°ê³¼ ì¡°ë¦½ (ë°±ì—”ë“œì— ì „ë‹¬í•  ë°ì´í„°)
        final_data = []
        if isinstance(summaries, list):
            for s in summaries:
                t_title = s.get('title', 'ê³µì§€')
                orig = next((p for p in targeted_data if t_title in p['title']), None)
                final_data.append({
                    "board_name": board['name'],
                    "title": t_title,
                    "summary": s.get('summary', ''),
                    "link": orig['link'] if orig else ""
                })

        return {
            "board": board['name'],
            "status": "SUCCESS",
            "data": final_data # ë°±ì—”ë“œê°€ ê°€ì ¸ê°ˆ í•µì‹¬ ë°ì´í„°
        }

    except Exception as e:
        LOG.exception(f"âŒ {board['name']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {"board": board['name'], "status": "ERROR", "data": []}
# í¬ë¡¤ë§ ëŒ€ìƒ ê²Œì‹œíŒ ì •ì˜ (ì½”ë“œ ìƒë‹¨ì— ì—†ë‹¤ë©´ ì¶”ê°€í•˜ì„¸ìš”)

if __name__ == "__main__":
    # 1. ë¡œê·¸ ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    LOG.info("ğŸš€ event.jsonì„ ì´ìš©í•œ ë¡œì»¬ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 2. event.json íŒŒì¼ ì½ê¸°
    event_path = "event.json"
    if os.path.exists(event_path):
        with open(event_path, "r", encoding="utf-8") as f:
            try:
                event_data = json.load(f)
                LOG.info("âœ… event.json íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except json.JSONDecodeError:
                LOG.error("âŒ event.json íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                sys.exit(1)
    else:
        # íŒŒì¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ìµœì†Œí•œì˜ ë”ë¯¸ ë°ì´í„°
        LOG.warning("âš ï¸ event.jsonì´ ì—†ì–´ ê¸°ë³¸ ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        event_data = {
            "userId": "test_user",
            "targetUrl": "https://info.korea.ac.kr/info/board/notice_under.do",
            "userProfile": {
                "summary": "ê³ ë ¤ëŒ€í•™êµ ì»´í“¨í„°í•™ê³¼ í•™ìƒ, AI í•´ì»¤í†¤ ë° ì¥í•™ê¸ˆì— ê´€ì‹¬ ìˆìŒ"
            },
            "config": {"language": "Korean"}
        }

    # 3. ì‹¤ì œ run í•¨ìˆ˜ ì‹¤í–‰
    try:
        # ìš°ë¦¬ê°€ ì •ì˜í•œ ì¸í’‹/ì•„ì›ƒí’‹ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” run í•¨ìˆ˜ í˜¸ì¶œ
        final_output = run(event_data)
        
        # 4. ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ìµœì¢… API ì‘ë‹µ ê²°ê³¼ (Output):")
        print(json.dumps(final_output, ensure_ascii=False, indent=2))
        print("="*50)
        
    except Exception as e:
        LOG.error(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")